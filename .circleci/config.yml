version: 2.1

commands:
  destroy-environment:
    description: Destroy back-end and front-end cloudformation stacks given a workflow ID  
    steps:
      - run:
          name: Destroy environments
          when: on_fail
          command: |
            aws cloudformation delete-stack --stack-name "udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}"
            aws cloudformation delete-stack --stack-name "udapeople-frontend-${CIRCLE_WORKFLOW_ID:0:7}"

jobs:
  build-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-build]
      - run:
          name: Build frontend
          command: |
            cd frontend
            npm install
            npm run build
      - save_cache:
          paths: [frontend/node_modules]
          key: frontend-build

  build-backend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [backend-build]
      - run:
          name: Build backend
          command: |
            cd backend
            npm install
            npm run build
      - save_cache:
          paths: [backend/node_modules]
          key: backend-build
  test-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys:
            - frontend-build
      - run:
          name: Test frontend
          command: |
            cd frontend
            npm install
            npm run test
  test-backend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys:
            - backend-build
      - run:
          name: Test backend
          command: |
            cd backend
            npm install
            npm run test
  scan-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys:
            - frontend-build
      - run:
          name: Scan frontend
          command: |
            cd frontend
            npm install
            npm audit fix --audit-level=critical --force
            npm audit --audit-level=critical
  scan-backend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys:
            - backend-build
      - run:
          name: Scan backend
          command: |
            cd backend
            npm install
            npm audit fix --audit-level=critical --force
            npm audit fix --audit-level=critical --force
            npm audit --audit-level=critical
  deploy-infrastructure:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run: yum install -y tar gzip
      - run:
          name: Ensure back-end infrastructure exists
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/backend.yml \
              --tags project=udapeople \
              --stack-name "udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"
      - run:
          name: Ensure front-end infrastructure exist
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/frontend.yml \
              --tags project=udapeople \
              --stack-name "udapeople-frontend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"
      - run:
          name: Add back-end ip to ansible inventory
          command: |
            echo "[web]" > ~/project/.circleci/ansible/inventory.txt
            aws ec2 describe-instances \
              --query 'Reservations[*].Instances[*].PublicIpAddress' \
              --output text >> ~/project/.circleci/ansible/inventory.txt
      - persist_to_workspace:
          root: ~/
          paths:
            - project/.circleci/ansible/inventory.txt
      - destroy-environment
  
  configure-infrastructure:
    docker:
      # Docker image here that supports Ansible
      - image: python:3.11-rc-alpine
    steps:
      # Checkout code from git
      - checkout
      # Add ssh keys with fingerprint
      - add_ssh_keys:
          fingerprints:
            - "50:ba:49:73:b6:57:fe:9b:a6:9a:97:f9:8b:92:75:93"
      # attach workspace
      - attach_workspace:
          at: ~/
      - run:
          name: Install dependencies
          command: apk add --no-cache --update aws-cli ansible tar gzip
      - run:
          name: Configure server
          command: |
            cd .circleci/ansible
            ansible-playbook -i inventory.txt configure-server.yml
      # Here's where you will add some code to rollback on failure

workflows:
  default:
    jobs:
      - build-frontend
      - build-backend
      - test-frontend:
          requires:
            - build-frontend
      - test-backend:
          requires:
            - build-backend
      - scan-frontend:
          requires: 
            - build-frontend
      - scan-backend:
          requires: 
            - build-backend
      - deploy-infrastructure:
          requires:
            - test-frontend 
            - test-backend 
            - scan-frontend 
            - scan-backend
          filters:
            branches:
              only: 
                - master
      - configure-infrastructure:
          requires:
            - deploy-infrastructure
